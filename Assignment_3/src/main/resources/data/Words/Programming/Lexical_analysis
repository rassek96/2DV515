lexical analysis from wikipedia free encyclopedia jump navigation search lexer redirects here for people with this name see lexer surname computer science lexical analysis process converting sequence characters such computer program web page into sequence tokens strings with identified meaning program that performs lexical analysis may be called lexer tokenizer scanner though scanner also used refer first stage lexer such lexer generally combined with parser which together analyze syntax programming languages web pages so forth contents applications lexeme token lexical grammar tokenization o scanner o evaluator lexer generator o list lexer generators phrase structure o line continuation o semicolon insertion o off-side rule context-sensitive lexing notes references external links applications lexer forms first phase compiler frontend modern processing generally done single pass lexers parsers are most often used for compilers but can be used for other computer language tools such prettyprinters linters lexing itself can be divided into two stages scanning which segments input sequence into groups categorizes these into token classes evaluating which converts raw input characters into processed value lexers are generally quite simple with most complexity deferred parser semantic analysis phases can often be generated by lexer generator notably lex derivatives however lexers can sometimes include some complexity such phrase structure processing make input easier simplify parser may be written partially completely by hand either support additional features for performance lexeme lexeme string characters which forms syntactic unit some authors for example just call this token using 'token' interchangeably represent string being tokenized also b token data structure resulting from putting this string through tokenization process note that usage word 'lexeme' computer science different from meaning word 'lexeme' linguistics lexeme computer science roughly corresponds what linguistics might be called word computer science 'word' has different meaning than meaning 'word' linguistics although some cases may be more similar morpheme token token structure representing lexeme that explicitly indicates its categorization for purpose parsing category tokens what linguistics might be called part-of-speech examples token categories may include identifier integer literal although set token categories differ different programming languages process forming tokens from input stream characters called tokenization consider this expression c programming language sum = + tokenized represented by following table lexeme token category sum identifier = assignment operator integer literal + addition operator integer literal end statement lexical grammar main article lexical grammar specification programming language often includes set rules lexical grammar which defines lexical syntax lexical syntax usually regular language with grammar rules consisting regular expressions they define set possible character sequences that are used form individual tokens lexemes lexer recognizes strings for each kind string found lexical program takes action most simply producing token two important common lexical categories are white space comments these are also defined grammar processed by lexer but may be discarded not producing any tokens considered non-significant at most separating two tokens if x instead ifx there are two important exceptions this firstly off-side rule languages that delimit blocks with indentation initial whitespace significant determines block structure generally handled at lexer level see phrase structure below secondly some uses lexers comments whitespace must be preserved – for examples prettyprinter also needs output comments some debugging tools may provide messages programmer showing original source code 1960s notably for algol whitespace comments were eliminated part line reconstruction phase initial phase compiler frontend but this separate phase has been eliminated these are now handled by lexer tokenization tokenization process demarcating possibly classifying sections string input characters resulting tokens are then passed on some other form processing process can be considered sub-task parsing input 'tokenization' has different meaning within field computer security take for example quick brown fox jumps over lazy dog string isn't implicitly segmented on spaces english speaker would do raw input characters must be explicitly split into tokens with given space delimiter ie matching string regular expression /\s{1}/ tokens could be represented xml quick brown fox jumps over lazy dog s-expression sentence word word quick word brown word fox word jumps word over word word lazy word dog when token class represents more than one possible lexeme lexer often saves enough information reproduce original lexeme so that can be used semantic analysis parser typically retrieves this information from lexer stores abstract syntax tree this necessary order avoid information loss case numbers identifiers tokens are identified based on specific rules lexer some methods used identify tokens include regular expressions specific sequences characters known flag specific separating characters called delimiters explicit definition by dictionary special characters including punctuation characters are commonly used by lexers identify tokens because their natural use written programming languages tokens are often categorized by character content by context within data stream categories are defined by rules lexer categories often involve grammar elements language used data stream programming languages often categorize tokens identifiers operators grouping symbols by data type written languages commonly categorize tokens nouns verbs adjectives punctuation categories are used for post-processing tokens either by parser by other functions program lexical analyzer generally does nothing with combinations tokens task left for parser for example typical lexical analyzer recognizes parentheses tokens but does nothing ensure that each matched with when lexer feeds tokens parser representation used typically enumerated list number representations for example identifier represented with assignment operator with addition operator with etc tokens are frequently defined by regular expressions which are understood by lexical analyzer generator such lex lexical analyzer either generated automatically by tool like lex hand-crafted reads stream characters identifies lexemes stream categorizes them into tokens this called tokenizing if lexer finds invalid token will report error following tokenizing parsing from there interpreted data may be loaded into data structures for general use interpretation compiling scanner first stage scanner usually based on finite-state machine fsm has encoded within information on possible sequences characters that can be contained within any tokens handles individual instances these character sequences are known lexemes for instance integer token may contain any sequence numerical digit characters many cases first non-whitespace character can be used deduce kind token that follows subsequent input characters are then processed one at time until reaching character that not set characters acceptable for that token this known maximal munch rule longest match rule some languages lexeme creation rules are more complicated may involve backtracking over previously read characters for example c single 'l' character not enough distinguish between identifier that begins with 'l' wide-character string literal evaluator lexeme however only string characters known be certain kind eg string literal sequence letters order construct token lexical analyzer needs second stage evaluator which goes over characters lexeme produce value lexeme's type combined with its value what properly constitutes token which can be given parser some tokens such parentheses do not really have values so evaluator function for these can return nothing only type needed similarly sometimes evaluators can suppress lexeme entirely concealing from parser which useful for whitespace comments evaluators for identifiers are usually simple literally representing identifier but may include some unstropping evaluators for integer literals may pass string on deferring evaluation semantic analysis phase may perform evaluation themselves which can be involved for different bases floating point numbers for simple quoted string literal evaluator only needs remove quotes but evaluator for escaped string literal itself incorporates lexer which unescapes escape sequences for example source code computer program string networthfuture = assets - liabilities might be converted into following lexical token stream note that whitespace suppressed special characters have no value name networthfuture equals openparenthesis name assets minus name liabilities closeparenthesis semicolon though possible sometimes necessary due licensing restrictions existing parsers if list tokens small write lexer by hand lexers are often generated by automated tools these tools generally accept regular expressions that describe tokens allowed input stream each regular expression associated with production rule lexical grammar programming language that evaluates lexemes matching regular expression these tools may generate source code that can be compiled executed construct state table for finite-state machine which plugged into template code for compilation execution regular expressions compactly represent patterns that characters lexemes might follow for example for english-based language name token might be any english alphabetical character underscore followed by any number instances ascii alphanumeric characters and/or underscores this could be represented compactly by string this means any character a-z a-z followed by more a-z a-z 0-9 regular expressions finite-state machines they generate are not powerful enough handle recursive patterns such n opening parentheses followed by statement followed by n closing parentheses they are not capable keeping count verifying that n same on both sides — unless you have finite set permissible values for n takes full-fledged parser recognize such patterns their full generality parser can push parentheses on stack then try pop them off see if stack empty at end see example sicp book lex programming tool its compiler designed generate code for fast lexical analysers based on formal description lexical syntax not generally considered sufficient for applications with complicated set lexical rules severe performance requirements for instance gnu compiler collection gcc uses hand-written lexers lexer generator see also parser generator lexers are often generated by lexer generator analogous parser generators such tools often come together most established lex paired with yacc parser generator free equivalents flex /bison these generators are form domain-specific language taking lexical specification – generally regular expressions with some markup – outputting lexer these tools yield very fast development which particularly important early development both get working lexer because language specification may be changing frequently further they often provide advanced features such pre- post-conditions which are hard program by hand however automatically generated lexer may lack flexibility thus may require some manual modification completely manually written lexer lexer performance concern optimization lexer worthwhile particularly stable languages where lexer run very frequently such c html lex/flex-generated lexers are reasonably fast but improvements two three times are possible using more tuned generators hand-written lexers are sometimes used but modern lexer generators produce faster lexers than most hand-coded ones lex/flex family generators uses table-driven approach which much less efficient than directly coded approach with latter approach generator produces engine that directly jumps follow-up states via goto statements tools like re2c quex have proven produce engines that are between two three times faster than flex produced engines general difficult hand-write analyzers that perform better than engines generated by these latter tools list lexer generators see also list parser generators antlr - can generate lexical analyzers parsers dfastar - generates dfa matrix table-driven lexers c++ flex - alternative variant classic lex c/c++ ragel - state machine lexer generator with output c c++ c objective-c d java go ruby following lexical analysers can handle unicode javacc - javacc generates lexical analyzers written java jflex - lexical analyzer generator for java quex - fast universal lexical analyzer generator for c c++ fslex - lexer generator for byte unicode character input for f phrase structure lexical analysis primarily segments input stream characters into tokens simply grouping characters into pieces categorizing them however lexing may be significantly more complex most simply lexers may omit tokens insert additional tokens omitting tokens notably whitespace comments very common when these are not needed by compiler less commonly additional tokens may be inserted this primarily done group tokens into statements statements into blocks simplify parser line continuation line continuation feature some languages where newline normally statement terminator most frequently ending line with backslash immediately followed by newline results line being continued – following line joined previous line this generally done lexer backslash newline are discarded rather than newline being tokenized examples include bash other shell scripts python semicolon insertion many languages use semicolon statement terminator most often this mandatory but some languages semicolon optional many contexts this primarily done at lexer level where lexer outputs semicolon into token stream despite one not being present input character stream known semicolon insertion automatic semicolon insertion these cases semicolons are part formal phrase grammar language but may not be found input text they can be inserted by lexer note that optional semicolons other terminators separators are also sometimes handled at parser level notably case trailing commas semicolons semicolon insertion feature bcpl its distant descendent go though not present b c semicolon insertion present javascript though rules are somewhat complicated much-criticized avoid bugs some recommend always using semicolons while others use initial semicolons known defensive semicolons at start potentially ambiguous statements semicolon insertion languages with semicolon-terminated statements line continuation languages with newline-terminated statements can be seen complementary semicolon insertion adds token even though newlines generally do not generate tokens while line continuation prevents token from being generated even though newlines generally do generate tokens off-side rule further information off-side rule off-side rule blocks determined by indentation can be implemented lexer python where increasing indentation results lexer outputting indent token decreasing indentation results lexer outputting dedent token these tokens correspond opening brace { closing brace } languages that use braces for blocks means that phrase grammar does not depend on whether braces indentation are used this requires that lexer hold state namely current indentation level thus can detect changes indentation when this changes thus lexical grammar not context-free – indent/dedent depend on contextual information previous indentation level context-sensitive lexing generally lexical grammars are context-free almost context-free do not require any looking back looking ahead backtracking which allows simple clean efficient implementation this also allows simple one-way communication from lexer parser without needing any information flowing back lexer there are exceptions however simple examples include semicolon insertion go which requires looking back one token concatenation consecutive string literals python which requires holding one token buffer before outputting see if next token another string literal off-side rule python which requires maintaining count indentation level indeed stack each indentation level these examples all only require lexical context while they complicate lexer somewhat they are invisible parser later phases more complicated example lexer hack c where token class sequence characters cannot be determined until semantic analysis phase since typedef names variable names are lexically identical but constitute different token classes – thus lexer hack lexer calls semantic analyzer say symbol table checks if sequence requires typedef name this case information has flow back not simply from parser but from semantic analyzer back lexer which complicates design notes older languages such algol initial stage was instead line reconstruction which performed unstropping removed whitespace comments fact had scannerless parsers without separate lexer these steps are now done part lexer references wwwcsmanacuk page compilers principles techniques & tools 2nd ed worldcat by aho lam sethi ullman quoted page compilers principles techniques & tools 2nd ed worldcat by aho lam sethi ullman quoted mitpressmitedu bumbulis p cowan d d mar–dec re2c more versatile scanner generator acm letters on programming languages systems 1–4 70–84 doi101145/176454176487 bash reference manual escape character python documentation lexical analysis explicit line joining effective go semicolons semicolons go golang-nuts rob 'commander' pike 12/10/09 python documentation lexical analysis indentation python documentation lexical analysis string literal concatenation compiling with c java pat terry isbn 032126360x algorithms + data structures = programs niklaus wirth isbn 0-13-022418-9 compiler construction niklaus wirth isbn 0-201-40353-6 sebesta r w concepts programming languages seventh edition pp boston pearson/addison-wesley external links yang w tsay chey-woei chan jien-tsai on applicability longest-match rule lexical analysis computer languages systems structures elsevier science 273–288 doi101016/s0096-05510200014-0 nsc 86-2213-e-009-021 nsc 86-2213-e-009-079 trim craig jan art tokenization developer works ibm word mention segmentation task analysis retrieved from https//enwikipediaorg/w/indexphptitle=lexicalanalysis&oldid=749582943 categories compiler construction interpreters computing programming language implementation parsing hidden categories all accuracy disputes articles with disputed statements from may all articles with unsourced statements articles with unsourced statements from april pages using isbn magic links navigation menu personal tools not logged talk contributions create account log namespaces article talk variants views read edit view history more search navigation main page contents featured content current events random article donate wikipedia wikipedia store interaction help about wikipedia community portal recent changes contact page tools what links here related changes upload file special pages permanent link page information wikidata item cite this page print/export create book download pdf printable version languages català čeština dansk deutsch ελληνικά español فارسی français 한국어 հայերեն hrvatski bahasa indonesia italiano עברית македонски nederlands 日本語 norsk bokmål polski português русский svenska தமிழ் українська tiếng việt 中文 edit links this page was last modified on november at text available under creative commons attribution-sharealike license additional terms may apply by using this site you agree terms use privacy policy wikipedia® registered trademark wikimedia foundation inc non-profit organization privacy policy about wikipedia disclaimers contact wikipedia developers cookie statement mobile view 