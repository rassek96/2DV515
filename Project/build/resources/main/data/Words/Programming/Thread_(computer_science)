thread computing from wikipedia free encyclopedia   redirected from thread computer science jump navigation search this article about concurrency concept for multithreading hardware see multithreading computer architecture for form code consisting entirely subroutine calls see threaded code for other uses see thread disambiguation this article includes list references but its sources remain unclear because has insufficient inline citations please help improve this article by introducing more precise citations december learn how when remove this template message process with two threads execution running on one processor computer science thread execution smallest sequence programmed instructions that can be managed independently by scheduler which typically part operating system implementation threads processes differs between operating systems but most cases thread component process multiple threads can exist within one process executing concurrently sharing resources such memory while different processes do not share these resources particular threads process share its executable code values its variables at any given time systems with single processor generally implement multithreading by time slicing central processing unit cpu switches between different software threads this context switching generally happens very often rapidly enough that users perceive threads tasks running parallel on multiprocessor multi-core system multiple threads can execute parallel with every processor core executing separate thread simultaneously on processor core with hardware threads separate software threads can also be executed concurrently by separate hardware threads threads made early appearance os/360 multiprogramming with variable number tasks mvt which context they were called tasks term thread has been attributed victor vyssotsky process schedulers many modern operating systems directly support both time-sliced multiprocessor threading operating system kernel allows programmers manipulate threads by exposing required functionality through system call interface some threading implementations are called kernel threads whereas light-weight processes lwp are specific type kernel thread that share same state information furthermore programs can have user-space threads when threading with timers signals other methods interrupt their own execution performing sort ad hoc time slicing contents threads vs processes single threading multithreading scheduling processes kernel threads user threads fibers o thread fiber issues + concurrency data structures + i/o scheduling models o kernel-level threading o n1 user-level threading o mn hybrid threading o hybrid implementation examples o fiber implementation examples programming language support practical multithreading see also notes references external links threads vs processes threads differ from traditional multitasking operating system processes that processes are typically independent while threads exist subsets process processes carry considerably more state information than threads whereas multiple threads within process share process state well memory other resources processes have separate address spaces whereas threads share their address space processes interact only through system-provided inter-process communication mechanisms context switching between threads same process typically faster than context switching between processes systems such windows nt os/2 are said have cheap threads expensive processes other operating systems there not so great difference except cost address space switch which on some architectures notably x86 results translation lookaside buffer tlb flush single threading computer programming single threading processing one command at time opposite single threading multithreading while has been suggested that term single threading misleading term has been widely accepted within functional programming community multithreading multithreading mainly found multitasking operating systems multithreading widespread programming execution model that allows multiple threads exist within context one process these threads share process's resources but are able execute independently threaded programming model provides developers with useful abstraction concurrent execution multithreading can also be applied one process enable parallel execution on multiprocessing system multithreaded applications have following advantages responsiveness multithreading can allow application remain responsive input one-thread program if main execution thread blocks on long-running task entire application can appear freeze by moving such long-running tasks worker thread that runs concurrently with main execution thread possible for application remain responsive user input while executing tasks background on other hand most cases multithreading not only way keep program responsive with non-blocking i/o and/or unix signals being available for gaining similar results faster execution this advantage multithreaded program allows operate faster on computer systems that have multiple central processing units cpus one more multi-core processors across cluster machines because threads program naturally lend themselves parallel execution assuming sufficient independence that they do not need wait for each other lower resource consumption using threads application can serve multiple clients concurrently using fewer resources than would need when using multiple process copies itself for example apache http server uses thread pools pool listener threads for listening incoming requests pool server threads for processing those requests better system utilization example file system using multiple threads can achieve higher throughput lower latency since data faster medium such cache memory can be retrieved by one thread while another thread retrieves data from slower medium such external storage with neither thread waiting for other finish simplified sharing communication unlike processes which require message passing shared memory mechanism perform inter-process communication ipc threads can communicate through data code files they already share parallelization applications looking use multicore multi-cpu systems can use multithreading split data tasks into parallel subtasks let underlying architecture manage how threads run either concurrently on one core parallel on multiple cores gpu computing environments like cuda opencl use multithreading model where dozens hundreds threads run parallel across data on large number cores multithreading has following drawbacks synchronization since threads share same address space programmer must be careful avoid race conditions other non-intuitive behaviors order for data be correctly manipulated threads will often need rendezvous time order process data correct order threads may also require mutually exclusive operations often implemented using semaphores order prevent common data from being simultaneously modified read while process being modified careless use such primitives can lead deadlocks thread crashes process illegal operation performed by thread crashes entire process therefore one misbehaving thread can disrupt processing all other threads application scheduling operating systems schedule threads either preemptively cooperatively preemptive multithreading generally considered superior approach allows operating system determine when context switch should occur disadvantage preemptive multithreading that system may make context switch at inappropriate time causing lock convoy priority inversion other negative effects which may be avoided by cooperative multithreading cooperative multithreading on other hand relies on threads themselves relinquish control once they are at stopping point this can create problems if thread waiting for resource become available until early 2000s most desktop computers had only one single-core cpu with no support for hardware threads although threads were still used on such computers because switching between threads was generally still quicker than full-process context switches intel added support for simultaneous multithreading pentium processor under name hyper-threading they introduced dual-core pentium d processor amd introduced dual-core athlon x2 processor processors embedded systems which have higher requirements for real-time behaviors might support multithreading by decreasing thread-switch time perhaps by allocating dedicated register file for each thread instead saving/restoring common register file processes kernel threads user threads fibers main articles process computing fiber computer science scheduling can be done at kernel level user level multitasking can be done preemptively cooperatively this yields variety related concepts at kernel level process contains one more kernel threads which share process's resources such memory file handles – process unit resources while thread unit scheduling execution kernel scheduling typically uniformly done preemptively less commonly cooperatively at user level process such runtime system can itself schedule multiple threads execution if these do not share data erlang they are usually analogously called processes while if they share data they are usually called user threads particularly if preemptively scheduled cooperatively scheduled user threads are known fibers different processes may schedule user threads differently user threads may be executed by kernel threads various ways one-to-one many-to-one many-to-many term light-weight process variously refers user threads kernel mechanisms for scheduling user threads onto kernel threads process heavyweight unit kernel scheduling creating destroying switching processes relatively expensive processes own resources allocated by operating system resources include memory for both code data file handles sockets device handles windows process control block processes are isolated by process isolation do not share address spaces file resources except through explicit methods such inheriting file handles shared memory segments mapping same file shared way – see interprocess communication creating destroying process relatively expensive resources must be acquired released processes are typically preemptively multitasked process switching relatively expensive beyond basic cost context switching due issues such cache flushing kernel thread lightweight unit kernel scheduling at least one kernel thread exists within each process if multiple kernel threads can exist within process then they share same memory file resources kernel threads are preemptively multitasked if operating system's process scheduler preemptive kernel threads do not own resources except for stack copy registers including program counter thread-local storage if any are thus relatively cheap create destroy thread switching also relatively cheap requires context switch saving restoring registers stack pointer but does not change virtual memory thus cache-friendly leaving tlb valid kernel can assign one thread each logical core system because each processor splits itself up into multiple logical cores if supports multithreading only supports one logical core per physical core if does not can swap out threads that get blocked however kernel threads take much longer than user threads be swapped threads are sometimes implemented userspace libraries thus called user threads kernel unaware them so they are managed scheduled userspace some implementations base their user threads on top several kernel threads benefit from multi-processor machines mn model this article term thread without kernel user qualifier defaults referring kernel threads user threads implemented by virtual machines are also called green threads user threads are generally fast create manage but cannot take advantage multithreading multiprocessing will get blocked if all their associated kernel threads get blocked even if there are some user threads that are ready run fibers are even lighter unit scheduling which are cooperatively scheduled running fiber must explicitly yield allow another fiber run which makes their implementation much easier than kernel user threads fiber can be scheduled run any thread same process this permits applications gain performance improvements by managing scheduling themselves instead relying on kernel scheduler which may not be tuned for application parallel programming environments such openmp typically implement their tasks through fibers closely related fibers are coroutines with distinction being that coroutines are language-level construct while fibers are system-level construct thread fiber issues concurrency data structures threads same process share same address space this allows concurrently running code couple tightly conveniently exchange data without overhead complexity ipc when shared between threads however even simple data structures become prone race conditions if they require more than one cpu instruction update two threads may end up attempting update data structure at same time find unexpectedly changing underfoot bugs caused by race conditions can be very difficult reproduce isolate prevent this threading application programming interfaces apis offer synchronization primitives such mutexes lock data structures against concurrent access on uniprocessor systems thread running into locked mutex must sleep hence trigger context switch on multi-processor systems thread may instead poll mutex spinlock both these may sap performance force processors symmetric multiprocessing smp systems contend for memory bus especially if granularity locking fine although threads seem be small step from sequential computation fact they represent huge step they discard most essential appealing properties sequential computation understandability predictability determinism threads model computation are wildly non-deterministic job programmer becomes one pruning that nondeterminism — the problem with threads edward lee uc berkeley i/o scheduling user thread fiber implementations are typically entirely userspace result context switching between user threads fibers within same process extremely efficient because does not require any interaction with kernel at all context switch can be performed by locally saving cpu registers used by currently executing user thread fiber then loading registers required by user thread fiber be executed since scheduling occurs userspace scheduling policy can be more easily tailored requirements program's workload however use blocking system calls user threads opposed kernel threads fibers can be problematic if user thread fiber performs system call that blocks other user threads fibers process are unable run until system call returns typical example this problem when performing i/o most programs are written perform i/o synchronously when i/o operation initiated system call made does not return until i/o operation has been completed intervening period entire process blocked by kernel cannot run which starves other user threads fibers same process from executing common solution this problem providing i/o api that implements synchronous interface by using non-blocking i/o internally scheduling another user thread fiber while i/o operation progress similar solutions can be provided for other blocking system calls alternatively program can be written avoid use synchronous i/o other blocking system calls sunos 4x implemented light-weight processes lwps netbsd 2x+ dragonfly bsd implement lwps kernel threads model sunos through sunos well netbsd netbsd implemented two level model multiplexing one more user level threads on each kernel thread mn model sunos later well netbsd eliminated user threads support returning model freebsd implemented mn model freebsd supported both mn users could choose which one should be used with given program using /etc/libmapconf starting with freebsd became default freebsd no longer supports mn model use kernel threads simplifies user code by moving some most complex aspects threading into kernel program does not need schedule threads explicitly yield processor user code can be written familiar procedural style including calls blocking apis without starving other threads however kernel threading may force context switch between threads at any time thus expose race hazards concurrency bugs that would otherwise lie latent on smp systems this further exacerbated because kernel threads may literally execute on separate processors parallel models kernel-level threading threads created by user correspondence with schedulable entities kernel are simplest possible threading implementation os/2 win32 used this approach from start while on linux usual c library implements this approach via nptl older linuxthreads this approach also used by solaris netbsd freebsd os x ios n1 user-level threading n1 model implies that all application-level threads map one kernel-level scheduled entity kernel has no knowledge application threads with this approach context switching can be done very quickly addition can be implemented even on simple kernels which do not support threading one major drawbacks however that cannot benefit from hardware acceleration on multithreaded processors multi-processor computers there never more than one thread being scheduled at same time for example if one threads needs execute i/o request whole process blocked threading advantage cannot be used gnu portable threads uses user-level threading does state threads mn hybrid threading mn maps some m number application threads onto some n number kernel entities virtual processors this compromise between kernel-level user-level n1 threading general mn threading systems are more complex implement than either kernel user threads because changes both kernel user-space code are required mn implementation threading library responsible for scheduling user threads on available schedulable entities this makes context switching threads very fast avoids system calls however this increases complexity likelihood priority inversion well suboptimal scheduling without extensive expensive coordination between userland scheduler kernel scheduler hybrid implementation examples scheduler activations used by netbsd native posix threads library implementation mn model opposed kernel userspace implementation model light-weight processes used by older versions solaris operating system marcel from pm2 project os for tera-cray mta-2 microsoft windows glasgow haskell compiler ghc for language haskell uses lightweight threads which are scheduled on operating system threads fiber implementation examples fibers can be implemented without operating system support although some operating systems libraries provide explicit support for them win32 supplies fiber api windows nt sp3 later ruby green threads netscape portable runtime includes user-space fibers implementation ribs2 programming language support ibm pl/if included support for multithreading called multitasking late 1960s this was continued optimizing compiler later versions ibm enterprise pl/i compiler introduced new model thread api neither version was part pl/i standard many programming languages support threading some capacity many implementations c c++ support threading provide access native threading apis operating system some higher level usually cross-platform programming languages such java python net framework languages expose threading developers while abstracting platform specific differences threading implementations runtime several other programming languages also try abstract concept concurrency threading from developer fully cilk openmp message passing interface mpi some languages are designed for sequential parallelism instead especially using gpus without requiring concurrency threads ateji px cuda few interpreted programming languages have implementations eg ruby mri for ruby cpython for python which support threading concurrency but not parallel execution threads due global interpreter lock gil gil mutual exclusion lock held by interpreter that can prevent interpreter from simultaneously interpreting applications code on two more threads at once which effectively limits parallelism on multiple core systems this limits performance mostly for processor-bound threads which require processor not much for i/o-bound network-bound ones other implementations interpreted programming languages such tcl using thread extension avoid gil limit by using apartment model where data code must be explicitly shared between threads tcl each thread has at one more interpreters event-driven programming hardware description languages such verilog have different threading model that supports extremely large numbers threads for modeling hardware practical multithreading standardized interface for thread implementation posix threads pthreads which set c-function library calls os vendors are free implement interface desired but application developer should be able use same interface across multiple platforms most unix platforms including linux support pthreads microsoft windows has its own set thread functions processh interface for multithreading like beginthread java provides yet another standardized interface over host operating system using java concurrency library javautilconcurrent multithreading libraries provide function call create new thread which takes function parameter concurrent thread then created which starts running passed function ends when function returns thread libraries also offer synchronization functions which make possible implement race condition-error free multithreading functions using mutexes condition variables critical sections semaphores monitors other synchronization primitives another paradigm thread usage that thread pools where set number threads are created at startup that then wait for task be assigned when new task arrives wakes up completes task goes back waiting this avoids relatively expensive thread creation destruction functions for every task performed takes thread management out application developer’s hand leaves library operating system that better suited optimize thread management for example frameworks like grand central dispatch threading building blocks programming models such cuda designed for data parallel computation array threads run same code parallel using only its id find its data memory essence application must be designed so that each thread performs same operation on different segments memory so that they can operate parallel use gpu architecture see also computer programming portal computing portal clone linux system call communicating sequential processes computer multitasking multi-core computing multithreading computer hardware non-blocking algorithm priority inversion protothreads simultaneous multithreading thread pool pattern thread safety win32 thread information block notes process switching changes virtual memory addressing causing invalidation thus flushing untagged translation lookaside buffer notably on x86 references lamport leslie september how make multiprocessor computer that correctly executes multiprocess programs pdf ieee transactions on computers c–28 690–691 doi 101109/tc19791675439 traffic control multiplexed computer system jerome howard saltzer doctor science thesis see footnote on page raúl menéndez doug lowe murach's cics for cobol programmer mike murach & associates p isbn 1-890774-09-x stephen r g fraser pro visual c++/cli net platform apress p isbn 1-4302-1053-2 peter william o'hearn r d tennent algol-like languages birkhäuser verlag p isbn 0-8176-3937-3 single-threading back future sergey ignatchenko overload erlang processes problem with threads edward lee uc berkeley january technical report no ucb/eecs-2006-1 multithreading solaris operating environment pdf archived from original pdf on february b c d gagne abraham silberschatz peter baer galvin greg operating system concepts 9th ed hoboken nj wiley pp 170–171 isbn createfiber msdn david r butenhof programming with posix threads addison-wesley isbn 0-201-63392-2 bradford nichols dick buttlar jacqueline proulx farell pthreads programming o'reilly & associates isbn 1-56592-115-1 charles j northrup programming with unix threads john wiley & sons isbn 0-471-13751-0 mark walmsley multi-threaded programming c++ springer isbn 1-85233-146-1 paul hyde java thread programming sams isbn 0-672-31585-8 bill lewis threads primer guide multithreaded programming prentice hall isbn 0-13-443698-9 steve kleiman devang shah bart smaalders programming with threads sunsoft press isbn 0-13-172389-8 pat villani advanced win32 programming files threads process synchronization harpercollins publishers isbn 0-87930-563-0 jim beveridge robert wiener multithreading applications win32 addison-wesley isbn 0-201-44234-5 thuan q pham pankaj k garg multithreaded programming with windows nt prentice hall isbn 0-13-120643-5 len dorfman marc j neuberger effective multithreading os/2 mcgraw-hill osborne media isbn 0-07-017841-0 alan burns andy wellings concurrency ada cambridge university press isbn 0-521-62911-x uresh vahalia unix internals new frontiers prentice hall isbn 0-13-101908-2 alan l dennis net multithreading manning publications company isbn 1-930110-54-5 tobin titus fabio claudio ferracchiati srinivasa sivakumar tejaswi redkar sandra gopikrishna c threading handbook peer information inc isbn 1-86100-829-5 tobin titus fabio claudio ferracchiati srinivasa sivakumar tejaswi redkar sandra gopikrishna visual basic net threading handbook wrox press inc isbn 1-86100-713-2 external links wikiversity has learning materials about processes threads at operating systems/process thread answers frequently asked questions for compprogrammingthreads what makes multi-threaded programming hard article query by slice parallel execute join thread pool pattern java by binildas c article free lunch over fundamental turn toward concurrency software by herb sutter article problem with threads by edward lee concepts multithreading contest - tool for testing multithreaded java applications by ibm debugging optimizing multithreaded openmp programs multithreading at dmoz multithreading solaris operating environment posix threads explained by daniel robbins c10k problem v t e parallel computing general distributed computing cloud computing high-performance computing levels bit instruction task data memory loop pipeline multithreading temporal simultaneous preemptive cooperative theory pram model analysis parallel algorithms amdahl's law gustafson's law cost efficiency karp–flatt metric slowdown speedup elements process thread fiber instruction window coordination multiprocessing memory coherency cache coherency cache invalidation barrier synchronization application checkpointing programming stream processing dataflow programming models o implicit parallelism o explicit parallelism o concurrency non-blocking algorithm hardware flynn's taxonomy o sisd o simd o misd o mimd dataflow architecture pipelined processor superscalar processor vector processor multiprocessor o symmetric o asymmetric memory o shared o distributed o distributed shared o uma o numa o coma massively parallel computer computer cluster grid computer apis ateji px boostthread charm++ cilk coarray fortran cuda dryad c++ amp global arrays mpi openmp opencl openhmpp openacc tpl plinq pvm posix threads raftlib upc tbb problems embarrassingly parallel software lockout scalability race condition deadlock livelock starvation deterministic algorithm parallel slowdown category parallel computing media related parallel computing at wikimedia commons v t e operating systems general advocacy comparison forensic engineering history hobbyist development list timeline usage share kernel architectures exokernel hybrid microkernel monolithic rump kernel unikernel components device driver loadable kernel module microkernel user space process management concepts context switch interrupt ipc process process control block thread time-sharing scheduling algorithms computer multitasking fixed-priority preemptive multilevel feedback queue preemptive round-robin shortest job next memory management resource protection bus error general protection fault memory protection paging protection ring segmentation fault virtual memory storage access file systems boot loader defragmentation device file file attribute inode journal partition virtual file system virtual tape library list amigaos android beos bsd chrome os cp/m dos hurd haiku illumos ios linux macintosh o classic mac os o macos minix morphos music/sp nos openvms orvyl os/2 osv pick qnx reactos risc os rsts/e rsx-11 rt-11 solaris tops-10/tops-20 tpf unix visi on vm/cms vs/9 windows xinu z/os miscellaneous concepts api computer network hal live cd live usb os shell o cli o gui o tui o vui pxe authority control lccn sh97002264 gnd 4377482-9 bnf cb133204714 data retrieved from https//enwikipediaorg/w/indexphptitle=threadcomputing&oldid=747769670 categories concurrent computing threads computing hidden categories articles lacking in-text citations from december all articles lacking in-text citations all articles with unsourced statements articles with unsourced statements from february articles with dmoz links commons category without link on wikidata wikipedia articles with lccn identifiers wikipedia articles with gnd identifiers wikipedia articles with bnf identifiers pages using isbn magic links navigation menu personal tools not logged talk contributions create account log namespaces article talk variants views read edit view history more search navigation main page contents featured content current events random article donate wikipedia wikipedia store interaction help about wikipedia community portal recent changes contact page tools what links here related changes upload file special pages permanent link page information wikidata item cite this page print/export create book download pdf printable version languages afrikaans العربية català čeština dansk deutsch eesti ελληνικά español فارسی français 한국어 bahasa indonesia íslenska italiano עברית magyar മലയാളം монгол nederlands 日本語 norsk bokmål polski português română русский shqip simple english slovenčina српски / srpski svenska ไทย türkçe українська 中文 edit links this page was last modified on november at text available under creative commons attribution-sharealike license additional terms may apply by using this site you agree terms use privacy policy wikipedia® registered trademark wikimedia foundation inc non-profit organization privacy policy about wikipedia disclaimers contact wikipedia developers cookie statement mobile view 